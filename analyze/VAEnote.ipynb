{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "import os\n",
    "def get_Sq2D_and_input_data(folder, parameters):\n",
    "\n",
    "    all_kappa, all_f, all_gL = [], [], []  # force related\n",
    "    all_Sq2D = []\n",
    "    qB = []\n",
    "    for L, run_num in parameters:\n",
    "        filename = f\"{folder}/obs_L{L}_random_run{run_num}.csv\"\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"File not found: {filename}\")\n",
    "            continue\n",
    "        data = np.genfromtxt(filename, delimiter=',', skip_header=1)\n",
    "        kappa, f, g = data[0, 2:5]\n",
    "        all_kappa.append(kappa)\n",
    "        all_f.append(f)\n",
    "        all_gL.append(g*L)\n",
    "\n",
    "        qB = data[2, 21:]\n",
    "        Sq2D = data[6:, 21:]\n",
    "        all_Sq2D_flatten.append(Sq2D.flatten())\n",
    "\n",
    "    all_feature = np.array([all_kappa, all_f, all_gL, all_R2, all_Rg2, all_Sxx, all_Syy, all_Szz, all_Sxy, all_Sxz, all_Syz])\n",
    "    all_feature_name = [\"kappa\", \"f\", \"gL\", \"R2\", \"Rg2\", \"Sxx\", \"Syy\", \"Szz\", \"Sxy\", \"Sxz\", \"Syz\"]\n",
    "    all_Sq2D_flatten = np.array(all_Sq2D_flatten)\n",
    "    qB = np.array(qB)\n",
    "    return all_feature.T, all_feature_name, all_Sq2D_flatten, qB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Encoder using 2 convolutional layers.\n",
    "# The input image is assumed to be of size (nq, nq)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, nq):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.nq = nq\n",
    "        self.conv = nn.Sequential(\n",
    "            # Input: [B, 1, nq, nq] -> Output: [B, 32, nq/2, nq/2]\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Output: [B, 32, nq/2, nq/2] -> [B, 64, nq/4, nq/4]\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Assuming nq is divisible by 4, the spatial size after conv layers is:\n",
    "        d2 = nq // 4\n",
    "        self.flatten_dim = 64 * d2 * d2\n",
    "        self.fc_mu = nn.Linear(self.flatten_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.flatten_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.conv(x)  # shape: [B, 64, nq/4, nq/4]\n",
    "        x = x.view(batch_size, -1)\n",
    "        return self.fc_mu(x), self.fc_logvar(x)\n",
    "\n",
    "# Decoder using 2 transposed convolutional layers.\n",
    "# It maps the latent vector back to an image of size (nq, nq)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, nq):\n",
    "        super(Decoder, self).__init__()\n",
    "        d2 = nq // 4\n",
    "        self.flatten_dim = 64 * d2 * d2\n",
    "        self.fc = nn.Linear(latent_dim, self.flatten_dim)\n",
    "        self.deconv = nn.Sequential(\n",
    "            # Reshape from [B, 64, d2, d2] -> Upsample: [B, 32, 2*d2, 2*d2]\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Upsample: [B, 32, 2*d2, 2*d2] -> [B, 1, 4*d2, 4*d2] which equals [B, 1, nq, nq]\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        batch_size = z.size(0)\n",
    "        d2 = int((self.flatten_dim // 64) ** 0.5)  # should equal nq//4\n",
    "        x = self.fc(z)\n",
    "        x = x.view(batch_size, 64, d2, d2)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "# VAE that combines the encoder and decoder.\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim, nq):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim, nq)\n",
    "        self.decoder = Decoder(latent_dim, nq)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "# Converter module that maps three model parameters (kappa, f, gL) into the latent space.\n",
    "class Converter(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Converter, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, params):\n",
    "        return self.net(params)\n",
    "\n",
    "# Example training functions\n",
    "\n",
    "def train_vae(vae, dataloader, num_epochs=50, lr=1e-3, device='cuda'):\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=lr)\n",
    "    vae.to(device)\n",
    "    vae.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:  # batch: [B, 1, nq, nq]\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = vae(batch)\n",
    "            recon_loss = F.mse_loss(recon, batch, reduction='sum')\n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            loss = recon_loss + kl_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader.dataset):.4f}\")\n",
    "\n",
    "def train_converter(converter, decoder, dataloader, num_epochs=30, lr=1e-3, device='cuda'):\n",
    "    # dataloader yields pairs (params, target_scattering) where:\n",
    "    #   params: [B, 3] and target_scattering: [B, 1, nq, nq]\n",
    "    optimizer = optim.Adam(converter.parameters(), lr=lr)\n",
    "    converter.to(device)\n",
    "    decoder.to(device)\n",
    "    decoder.eval()  # freeze the decoder\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for params, target in dataloader:\n",
    "            params, target = params.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            z = converter(params)\n",
    "            output = decoder(z)\n",
    "            loss = F.mse_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"[Converter] Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "def generate_scattering(converter, decoder, polymer_params, device='cuda'):\n",
    "    # polymer_params: [B, 3] for (kappa, f, gL)\n",
    "    converter.to(device)\n",
    "    decoder.to(device)\n",
    "    converter.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        z = converter(polymer_params.to(device))\n",
    "        return decoder(z)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    latent_dim = 64\n",
    "    nq = 64  # Set the input image size (nq x nq); can be any value divisible by 4.\n",
    "    vae = VAE(latent_dim, nq)\n",
    "    converter = Converter(latent_dim)\n",
    "\n",
    "    # Assume you have:\n",
    "    # - vae_dataloader that provides [B, 1, nq, nq] scattering images.\n",
    "    # - reg_dataloader that provides (polymer_params, target_scattering) pairs.\n",
    "    #\n",
    "    # To train the VAE:\n",
    "    # train_vae(vae, vae_dataloader, num_epochs=50, lr=1e-3, device='cuda')\n",
    "    #\n",
    "    # Freeze the decoder from the pretrained VAE:\n",
    "    # pretrained_decoder = vae.decoder\n",
    "    #\n",
    "    # Train the converter:\n",
    "    # train_converter(converter, pretrained_decoder, reg_dataloader, num_epochs=30, lr=1e-3, device='cuda')\n",
    "    #\n",
    "    # To generate a scattering image for new polymer parameters:\n",
    "    # polymer_params = torch.tensor([[0.8, 1.2, 0.5]])  # shape: [1, 3] for (kappa, f, gL)\n",
    "    # scattering_image = generate_scattering(converter, pretrained_decoder, polymer_params, device='cuda')\n",
    "    # The scattering_image will have shape [1, 1, nq, nq].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
